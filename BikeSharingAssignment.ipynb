{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement:\n",
    "\n",
    "A US bike-sharing provider `BoomBikes` has a daily dataset on the rental bikes based on various environmental and seasonal settings. It wishes to use this data to understand the factors affecting the demand for these shared bikes in the American market and come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown due to corona pandemic comes to an end.\n",
    "\n",
    "**the company wants to know**:\n",
    "\n",
    "\n",
    "- Which variables are significant in predicting the demand for shared bikes.\n",
    "\n",
    "\n",
    "- How well those variables describe the bike demands\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution is divided into the following main sections: \n",
    "- Data understanding/exploration\n",
    "- Data Visualisation \n",
    "- Data preparation/cleaning\n",
    "- Model building and evaluation\n",
    "- Residual Analysis (verifying the assumptions of Linear Model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Understanding/Exploration\n",
    "\n",
    "Let's first import the required libraries and have a look at the dataset and understand the size, attribute names etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "bike = pd.read_csv(\"day.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the first few rows\n",
    "bike.head(3).append(bike.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the number of rows and columns in the dataset\n",
    "bike.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Understanding the feature names in the dataset\n",
    "print(bike.columns)\n",
    "bike.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the dataset: 730 rows, 16 columns, no null values\n",
    "print(bike.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting insights of the features\n",
    "bike.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Data Dictionary and parts of Data Preparation\n",
    "\n",
    "The data dictionary contains the meaning of various attributes; some of which are explored and manipulated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning string values to different seasons instead of numeric values. These numeric values may misindicate some order to it.\n",
    "print(bike['season'].value_counts())\n",
    "# 1=spring\n",
    "bike.loc[(bike['season'] == 1) , 'season'] = 'spring'\n",
    "\n",
    "# 2=summer\n",
    "bike.loc[(bike['season'] == 2) , 'season'] = 'summer'\n",
    "\n",
    "# 3=fall\n",
    "bike.loc[(bike['season'] == 3) , 'season'] = 'fall'\n",
    "\n",
    "# 4=winter\n",
    "bike.loc[(bike['season'] == 4) , 'season'] = 'winter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the conversion is done properly or not and getting data count on the basis of season\n",
    "bike['season'].astype('category').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year (0: 2018, 1:2019)\n",
    "bike['yr'].astype('category').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning string values to different months instead of numeric values which may misindicate some order to it.\n",
    "# A function has been created to map the actual numbers to categorical levels.\n",
    "# Check the data before change\n",
    "print(bike['mnth'].astype('category').value_counts())\n",
    "def object_map(x):\n",
    "    return x.map({1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul',8: 'Aug',9: 'Sept',10: 'Oct',11: 'Nov',12: 'Dec'})\n",
    "\n",
    "# Applying the function to the two columns\n",
    "bike[['mnth']] = bike[['mnth']].apply(object_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the conversion is done properly or not and getting data count on the basis of month\n",
    "bike['mnth'].astype('category').value_counts(ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether day is a holiday or not (0: No, 1: Yes)\n",
    "bike['holiday'].astype('category').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning string values to weekdays instead of numeric values. These values may misindicate some order to it.\n",
    "# A function has been created to map the actual numbers to categorical levels.\n",
    "def str_map(x):\n",
    "    return x.map({1: 'Wed', 2: 'Thur', 3: 'Fri', 4: 'Sat', 5: 'Sun', 6: 'Mon', 0: 'Tue'})\n",
    "\n",
    "# Applying the function to the two columns\n",
    "bike[['weekday']] = bike[['weekday']].apply(str_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the conversion is done properly or not and getting data count on the basis of weekdays\n",
    "bike['weekday'].astype('category').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if a day is neither weekend nor a holiday it takes the value 1, otherwise 0\n",
    "bike['workingday'].astype('category').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Replacing long weathersit names into string values for better readability and understanding\n",
    "\n",
    "# 1-Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "bike.loc[(bike['weathersit'] == 1) , 'weathersit'] = 'A'\n",
    "\n",
    "# 2-Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "bike.loc[(bike['weathersit'] == 2) , 'weathersit'] = 'B'\n",
    "\n",
    "# 3-Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "bike.loc[(bike['weathersit'] == 3) , 'weathersit'] = 'C'\n",
    "\n",
    "# 4-Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "bike.loc[(bike['weathersit'] == 4) , 'weathersit'] = 'D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extracting the type of weather situations present in the data\n",
    "print(bike['weathersit'].unique())\n",
    "print(bike['weathersit'].astype('category').value_counts(ascending = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting date to datetime format\n",
    "print(\"As is:\",bike['dteday'].dtypes)\n",
    "bike['dteday']=bike['dteday'].astype('datetime64')\n",
    "print(\"Now:\", bike['dteday'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: 1. Data Understanding/Exploration\n",
    "\n",
    "In this section, we have analysed the given dataset w.r.to it's structure. In the process, we have changed the data types of few columns and also changed some of the values based on Data Dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Visualisation\n",
    "\n",
    "Let's now spend some time doing what is arguably the most important step - **Data Content Analysis**.\n",
    "- Understanding the distribution of various numeric variables \n",
    "- If there is some obvious multicollinearity going on, this is the first place to catch it\n",
    "- Here's where you'll also identify if some predictors directly have a strong association with the outcome variable\n",
    "\n",
    "We'll visualise our data using `matplotlib` and `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp: temperature in Celsius\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.distplot(bike['temp'],bins = 15, color = 'b').set(title='Temperature in Celsius')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeling temperature\n",
    "sns.distplot(bike['atemp']).set(title='Feeling Temperature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# humidity\n",
    "sns.distplot(bike['hum'], color = 'g').set(title='Humidity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wind speed\n",
    "sns.distplot(bike['windspeed'], color = 'r').set(title='Wind Speed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable: count of total rental bikes including both casual and registered\n",
    "sns.distplot(bike['cnt'], bins = 10).set(title='Count of Rented Bikes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: 2018, 1: 2019\n",
    "sns.barplot(x='yr', y='cnt', data=bike)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='weekday', y='cnt', data=bike)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holiday - 0: No, 1: Yes\n",
    "sns.barplot(x='holiday', y='cnt', data=bike)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weathersit\n",
    "# 'A'- Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "# 'B'- Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "# 'C'- Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "# 'D'- Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "\n",
    "sns.barplot(x='weathersit', y='cnt', data=bike)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All categorical variables in the dataset\n",
    "bike_categorical = bike.select_dtypes(exclude=['float64','datetime64','int64'])\n",
    "print(\"Only categorical varibales:\", bike_categorical.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bike_categorical.head(3).append(bike_categorical.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising Categorical Variables\n",
    "\n",
    "As you might have noticed, there are a few categorical variables as well. Let's make a boxplot for some of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))  \n",
    "plt.subplot(3,3,1)\n",
    "sns.boxplot(x = 'season', y = 'cnt', data = bike)\n",
    "plt.subplot(3,3,2)\n",
    "sns.boxplot(x = 'holiday', y = 'cnt', data = bike)\n",
    "plt.subplot(3,3,3)\n",
    "sns.boxplot(x = 'workingday', y = 'cnt', data = bike)\n",
    "plt.subplot(3,3,4)\n",
    "sns.boxplot(x = 'weathersit', y = 'cnt', data = bike)\n",
    "plt.subplot(3,3,5)\n",
    "sns.boxplot(x = 'mnth', y = 'cnt', data = bike)\n",
    "plt.subplot(3,3,6)\n",
    "sns.boxplot(x = 'weekday', y = 'cnt', data = bike)\n",
    "plt.subplot(3,3,7)\n",
    "sns.boxplot(x = 'yr', y = 'cnt', data = bike)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising Numeric Variables\n",
    "\n",
    "Let's make a pairplot of all the numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting \"casual\",\"registered\" and \"cnt\" Integer/discreate variables to float. \n",
    "# This step is performed to seperate out categorical variables like 'yr','holiday','workingday' which have binary values in them\n",
    "Int_Var_List = [\"casual\",\"registered\",\"cnt\"]\n",
    "\n",
    "for var in Int_Var_List:\n",
    "    bike[var] = bike[var].astype(\"float\")\n",
    "bike[Int_Var_List].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All numeric variables in the dataset\n",
    "bike_numeric = bike.select_dtypes(include=['float64'])\n",
    "bike_numeric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can better plot correlation matrix between variables to know the exact values of correlation between them. Also, a heatmap is pretty useful to visualise multiple correlations in one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "cor = bike_numeric.corr()\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#removing atemp as it is highly correlated with temp\n",
    "bike.drop('atemp',axis=1,inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bike.shape)\n",
    "print(bike.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: 2. Data Visualization\n",
    "\n",
    "In this section, we have looked at the actual data content.We plotted few distribution plots and a correleation Matrix and heatmap for numerical varibales. We also removed one of the features \"atemp\" as it was highly correlated with the another varibales \"temp\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation \n",
    "\n",
    "\n",
    "#### Data Preparation\n",
    "\n",
    "Let's now prepare the data and build the model.\n",
    "Note that we had not included 'yr', 'mnth', 'holiday', 'weekday' and 'workingday' as object variables in the initial data exploration steps so as to avoid too many dummy variables creation. They have binary values: 0s and 1s in them which have specific meanings associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset all categorical variables\n",
    "bike_categorical=bike.select_dtypes(include=['object'])\n",
    "bike_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Variables\n",
    "The variable `season`,`mnth`,`weekday` and `weathersit` have different levels. We need to convert these levels into integers. \n",
    "\n",
    "For this, we will use something called `dummy variables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into dummies\n",
    "bike_dummies = pd.get_dummies(bike_categorical, drop_first=True)\n",
    "bike_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop categorical variable columns\n",
    "bike = bike.drop(list(bike_categorical.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dummy variables with the original dataframe\n",
    "bike = pd.concat([bike, bike_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the first few rows\n",
    "bike.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Column 'instant'[Record Index] and 'dteday'[Date]\n",
    "\n",
    "print(len(bike.instant.unique()))\n",
    "print(len(bike.dteday.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'instant' and 'dteday' column as they of not any use to us for the analysis\n",
    "bike=bike.drop(['instant','dteday'], axis = 1, inplace = False)\n",
    "bike.head(3).append(bike.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: 3. Data Preparation\n",
    "\n",
    "The main task done in this section are:\n",
    "- create the respective dummy variables\n",
    "- delete the irrelevant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Building and Evaluation\n",
    "\n",
    "Let's start building the model. The first step to model building is the usual test-train split. So let's perform that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before splitting, make a copy of the cleaned data frame\n",
    "bike_c = bike.copy()\n",
    "print(bike.shape)\n",
    "print(bike_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(bike, train_size=0.7, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "# from sklearn.preprocessing import StandardScaler - in case you want to use Standardization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_numeric.columns  # created in previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\n",
    "var = ['temp', 'hum', 'windspeed','casual','registered','cnt']\n",
    "\n",
    "df_train[var] = scaler.fit_transform(df_train[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train[var].describe()\n",
    "\n",
    "# Please note that min is 0 and max. is 1 now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the variables have been appropriately scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's check the correlation coefficients to see which variables are highly correlated\n",
    "plt.figure(figsize = (30, 30))\n",
    "sns.heatmap(df_train.corr(), annot = True, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed, `temp` seems to the correlated to `cnt` the most, after 'casual' and 'registered'. Let's see a pairplot for `temp` vs `cnt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20)) \n",
    "\n",
    "plt.subplot(6,6,1)\n",
    "plt.scatter(df_train.temp, df_train.cnt)\n",
    "\n",
    "plt.subplot(6,6,2)\n",
    "plt.scatter(df_train.casual, df_train.cnt)\n",
    "\n",
    "plt.subplot(6,6,3)\n",
    "plt.scatter(df_train.registered, df_train.cnt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing into X and Y sets for the model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 'casual' and 'registered' as together they add up to cnt\n",
    "y_train = df_train.pop('cnt')\n",
    "X_train = df_train.drop([\"casual\",\"registered\"],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the first model with all the features\n",
    "\n",
    "Let's now build our first model with all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X_train_lm = sm.add_constant(X_train)\n",
    "lr_model1 = sm.OLS(y_train, X_train_lm).fit()\n",
    "\n",
    "lr_model1.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model using sklearn Linear Regression\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Fit a line\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept\n",
    "print(lm.coef_)\n",
    "print(lm.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# getting the model summary from statsmodel\n",
    "lr_model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has an Adjusted R-squared value of **84.5%** which seems pretty good. But let's see if we can reduce the number of features and exclude those which are not much relevant in explaining the target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building Using RFE\n",
    "\n",
    "Now, you have close to 28 features. It is obviously not recommended to manually eliminate these features. So let's now build a model using recursive feature elimination to select features. We'll first start off with an arbitrary number of features (15 seems to be a good number to begin with), and then use the `statsmodels` library to build models using the shortlisted features (this is also because `SKLearn` doesn't have `Adjusted R-squared` that `statsmodels` has)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# RFE with 15 features\n",
    "lm = LinearRegression()\n",
    "rfe1 = RFE(lm, 15)\n",
    "\n",
    "# Fit with 15 features\n",
    "rfe1.fit(X_train, y_train)\n",
    "\n",
    "# Print the boolean results\n",
    "print(rfe1.support_)           \n",
    "print(rfe1.ranking_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building and Evaluation \n",
    "\n",
    "Let's now check the summary of this model using `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statsmodels\n",
    "import statsmodels.api as sm  \n",
    "\n",
    "# Subset the features selected by rfe1\n",
    "col1 = X_train.columns[rfe1.support_]\n",
    "\n",
    "# Subsetting training data for 15 selected columns\n",
    "X_train_rfe1 = X_train[col1]\n",
    "\n",
    "# Add a constant to the model\n",
    "X_train_rfe1 = sm.add_constant(X_train_rfe1)\n",
    "X_train_rfe1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fitting the model with 15 variables\n",
    "lm1 = sm.OLS(y_train, X_train_rfe1).fit()   \n",
    "print(lm1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the new model built on the selected features doesn't show much dip in the accuracy in comparison to the model which was built on all the features. It has gone from **84.5%** to **84.4%**. This is indeed a good indication to proceed with these selected features.\n",
    "\n",
    "But let's check for the multicollinearity among these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the VIF values of the feature variables. \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=X_train_rfe1.drop('const',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs except for the constant\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = a.columns\n",
    "vif['VIF'] = [variance_inflation_factor(a.values, i) for i in range(a.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE with 8 features\n",
    "lm = LinearRegression()\n",
    "rfe2 = RFE(lm, 8)\n",
    "\n",
    "# Fit with 7 features\n",
    "rfe2.fit(X_train, y_train)\n",
    "\n",
    "# Print the boolean results\n",
    "print(rfe2.support_)           \n",
    "print(rfe2.ranking_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statsmodels\n",
    "import statsmodels.api as sm  \n",
    "\n",
    "# Subset the features selected by rfe1\n",
    "col1 = X_train.columns[rfe2.support_]\n",
    "\n",
    "# Subsetting training data for 7 selected columns\n",
    "X_train_rfe2 = X_train[col1]\n",
    "\n",
    "# Add a constant to the model\n",
    "X_train_rfe2 = sm.add_constant(X_train_rfe2)\n",
    "X_train_rfe2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model with 10 variables\n",
    "lm2 = sm.OLS(y_train, X_train_rfe2).fit()   \n",
    "print(lm2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the VIF for these selected features and decide further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=X_train_rfe2.drop('const',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs except for the constant\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = b.columns\n",
    "vif['VIF'] = [variance_inflation_factor(b.values, i) for i in range(b.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model summary above, all the variables have p-value < 0.05 and from the p-value perspective, all variables seem significant. But notice that there are a few variables which have VIF > 5. We need to deal with these variables carefully.\n",
    "\n",
    "So let's try removing 'hum' first having the maximum VIF and then check for it again. Dropping this variable may result in a change in other VIFs which are high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's drop the 'hum' column\n",
    "X_train_rfe2.drop(\"hum\",axis=1,inplace=True)\n",
    "X_train_rfe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe2 = sm.add_constant(X_train_rfe2)\n",
    "\n",
    "# Now that we have removed one variable, let's fit the model with 6 variables\n",
    "lm3 = sm.OLS(y_train, X_train_rfe2).fit()   \n",
    "print(lm3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to be doing a good job. Let's also quickly take a look at the VIF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=X_train_rfe2.drop('const',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs except for the constant\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = c.columns\n",
    "vif['VIF'] = [variance_inflation_factor(c.values, i) for i in range(c.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Prediction on train data set\n",
    "y_train_cnt = lm3.predict(X_train_rfe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cnt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Mean of Error terms\n",
    "residuals = y_train - y_train_cnt\n",
    "mean_residuals = np.mean(residuals)\n",
    "print(\"Mean of Residuals {}\".format(mean_residuals))\n",
    "residuals.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We need to check if the residuals or error terms are normally distributed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the error or residuals terms terms\n",
    "fig = plt.figure()\n",
    "sns.distplot((y_train - y_train_cnt), bins = 20, color = 'r')\n",
    "# Plot heading\n",
    "fig.suptitle('Error Terms', fontsize = 20)    \n",
    "# Give the X-label\n",
    "plt.xlabel('Errors', fontsize = 18)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "p = sns.scatterplot(y_train_cnt,residuals, color = \"r\")\n",
    "plt.xlabel('y_pred', color = 'r')\n",
    "plt.ylabel('Residuals', color = 'b')\n",
    "plt.ylim(-1,1)\n",
    "plt.xlim(0,1)\n",
    "p = sns.lineplot([0,1],[0,0],color='g')\n",
    "p = plt.title('Homoscedasticity Check')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error terms are independent of each other**\n",
    "\n",
    "That means there should not be any auto-correlation between error terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "p = sns.lineplot(y_train_cnt,residuals,marker='o',color='purple')\n",
    "plt.xlabel('y_pred')\n",
    "plt.ylabel('Residuals')\n",
    "plt.ylim(-1,1)\n",
    "plt.xlim(0,1)\n",
    "p = sns.lineplot([0,1],[0,0],color='green')\n",
    "p = plt.title('Residuals vs fitted values plot for autocorrelation check')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "We would first need to scale the test set as well. So let's start with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's recall the set of variables which are to be scaled\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test holds the test dataset for us\n",
    "df_test[var] = scaler.transform(df_test[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'df_test' set into X and y after scaling\n",
    "y_test = df_test.pop('cnt')\n",
    "X_test = df_test.drop([\"casual\",\"registered\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the list 'col2' which had the 7 variables RFE had selected\n",
    "col2=c.columns\n",
    "print(len(col2))\n",
    "col2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's subset these columns and create a new dataframe 'X_test_rfe1'\n",
    "X_test_rfe2 = X_test[col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to the test set created\n",
    "X_test_rfe2 = sm.add_constant(X_test_rfe2)\n",
    "X_test_rfe2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using our final model: lm3\n",
    "y_pred = lm3.predict(X_test_rfe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting y_test and y_pred to understand the spread\n",
    "\n",
    "fig = plt.figure()\n",
    "#plt.scatter(y_test, y_pred)\n",
    "sns.regplot(x=y_test,y=y_pred,ci=None,color ='purple');\n",
    "fig.suptitle('y_test vs y_pred', fontsize = 16, color = 'green')              # Plot heading \n",
    "plt.xlabel('y_test', fontsize = 16, color = 'blue')                          # X-label\n",
    "plt.ylabel('y_pred', fontsize = 16, color = 'blue')                          # Y-label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, it's evident that the model is doing well on the test set as well. Let's also check the R-squared and more importantly, the adjusted R-squared value for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_score for 8 variables on test dataset and it's predcition\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score = r2_score(y_test, y_pred)\n",
    "print(\"r2 score is: \",r2_score)\n",
    "n = len(X_test)\n",
    "p = 7\n",
    "print(\"and Adjusted r2 score is: \",1-(1-r2_score)*(n-1)/(n-p-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for the model with 7 variables, the r-squared on training and test data is about 79.6% and 78.35% respectively. The **Adjusted r-squared** on the train and test set is 79.3% and 77.63% respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the correlations between the final predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(bike[col2].corr(), cmap=\"YlGnBu\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest model that we could build. The final predictors seem to have fairly low correlations. \n",
    "\n",
    "Thus, the final model consists of the **7 variables** mentioned above.One can go ahead with this model and use it for predicting count of daily bike rentals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(lm3.params))\n",
    "print(abs(lm3.params).sort_values(ascending = False)) # It's magnitude of the coefficinets that matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suggestion to BoomBikes Management\n",
    "\n",
    "We can conclude from our model that below three features are the most influential features for BIke Rentals:\n",
    "- Temperature : with coefficient `0.42`\n",
    "- Weather C [3-Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds ]:  with coefficient `0.24`\n",
    "- Year: with coefficient `0.23`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: 4. Model Building and Evaluation\n",
    "\n",
    "In this section, we build our first model considering all the variables and then we made an informed choice based on VIF and p-values and finally made our baseline model with 7 variables. We also plotted the error term to check if they are normally distributed. Our regression line also looks pretty clustered around the central line."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
